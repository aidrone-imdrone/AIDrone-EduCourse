# **YOLO ê¸°ë°˜ íŠ¹ì • ê°ì²´ ì¶”ì  í”„ë¡œì íŠ¸**

## **1. ê°œìš”**
### **í”„ë¡œì íŠ¸ ëª©í‘œ**
ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” **YOLO (You Only Look Once) ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ê°ì²´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ëŠ” ë“œë¡  ì‹œìŠ¤í…œ**ì„ êµ¬í˜„í•©ë‹ˆë‹¤. YOLO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì—ì„œ íŠ¹ì • ê°ì²´(ì˜ˆ: ì‚¬ëŒ, ìë™ì°¨ ë“±)ë¥¼ ê°ì§€í•˜ê³ , ì‚¬ìš©ìê°€ ì§ì ‘ ì„ íƒí•œ ê°ì²´ë¥¼ ë”°ë¼ê°€ë„ë¡ í•©ë‹ˆë‹¤.

### **í•„ìš”í•œ ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬**
- **Python**: í”„ë¡œê·¸ë˜ë° ì–¸ì–´
- **OpenCV**: ì‹¤ì‹œê°„ ì˜ìƒ ì²˜ë¦¬
- **YOLOv4/v5**: ê°ì²´ íƒì§€ ëª¨ë¸
- **pyaidrone**: ë“œë¡  ì œì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **NumPy**: ë°ì´í„° ì²˜ë¦¬ ë° ë°°ì—´ ì—°ì‚°
- **Raspberry Pi Zero 2W**: ë“œë¡ ì˜ ì—°ì‚° ì¥ì¹˜
- **Camera Module**: ì˜ìƒ ë°ì´í„° ìˆ˜ì§‘

### **ê¸°ëŠ¥ ê°œìš”**
- **YOLOë¥¼ ì´ìš©í•œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€**
- **ì‚¬ìš©ìê°€ ì„ íƒí•œ ê°ì²´ë¥¼ ì¶”ì **
- **ë“œë¡ ì´ ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ì—¬ ì´ë™**
- **YOLOì— ì—†ëŠ” ê°ì²´ë¥¼ ì¶”ê°€ í•™ìŠµí•˜ì—¬ ì¸ì‹ ê°€ëŠ¥**

---

## **2. í™˜ê²½ ì„¤ì •**
### **í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
pip install opencv-python numpy torch torchvision pyyaml pyaidrone
```

### **YOLO ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
[YOLOv5 ê³µì‹ ì €ì¥ì†Œ](https://github.com/ultralytics/yolov5)ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
```bash
git clone https://github.com/ultralytics/yolov5
cd yolov5
pip install -r requirements.txt
```

---

## **3. YOLOì— ìƒˆë¡œìš´ ê°ì²´ ì¶”ê°€í•˜ê¸°**
ê¸°ë³¸ YOLO ëª¨ë¸ì— ì—†ëŠ” ê°ì²´ë¥¼ ì¶”ê°€ë¡œ í•™ìŠµí•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## **2. Collecting Data**
The first step is to gather images of the object you want YOLO to recognize.

### **Step 1: Capture or Download Images**
- Use your own camera to take pictures from different angles.
- Find public datasets (e.g., [Google Open Images](https://storage.googleapis.com/openimages/web/index.html)).
- Save images in a new folder, e.g., `dataset/images/`.

### **Step 2: Resize Images**
To ensure better training performance, resize images to a uniform size (e.g., 640x640 pixels):
```python
import cv2
import os

dataset_path = "dataset/images/"
output_path = "dataset/resized/"
os.makedirs(output_path, exist_ok=True)

for file in os.listdir(dataset_path):
    img = cv2.imread(os.path.join(dataset_path, file))
    img_resized = cv2.resize(img, (640, 640))
    cv2.imwrite(os.path.join(output_path, file), img_resized)
```

---

## **3. Labeling Data for YOLO**
YOLO requires labeled data in a specific format (`.txt` files).

### **Step 1: Install Labeling Tool**
Use [LabelImg](https://github.com/heartexlabs/labelImg) to annotate images:
```bash
pip install labelImg
labelImg
```

### **Step 2: Annotate Images**
- Open `LabelImg`, select the image folder (`dataset/resized/`).
- Label each object and save as YOLO format.
- This will create `.txt` annotation files alongside images.

---

## **4. Preparing Data for Training**
### **Step 1: Organize Data Folders**
Create the following structure:
```
yolo_custom_dataset/
  â”œâ”€â”€ images/
  â”‚   â”œâ”€â”€ train/   # Training images
  â”‚   â”œâ”€â”€ val/     # Validation images
  â”œâ”€â”€ labels/
  â”‚   â”œâ”€â”€ train/   # Training labels
  â”‚   â”œâ”€â”€ val/     # Validation labels
```
Move `80%` of images to `train/` and `20%` to `val/`.

### **Step 2: Create `custom_data.yaml`**
```yaml
train: /path_to_dataset/yolo_custom_dataset/images/train
val: /path_to_dataset/yolo_custom_dataset/images/val
nc: 1  # Number of classes
names: ['custom_object']  # Name of your object
```

---

## **5. Training the YOLO Model**
Use the YOLOv5 training script with your custom dataset:
```bash
python train.py --img 640 --batch 16 --epochs 50 --data custom_data.yaml --weights yolov5s.pt
```
- `--img 640`: Image size
- `--batch 16`: Batch size (adjust based on RAM)
- `--epochs 50`: Number of training iterations
- `--data custom_data.yaml`: Path to dataset configuration
- `--weights yolov5s.pt`: Pretrained model to start training from

---

## **6. Testing the Trained Model**
After training, the model is saved in `runs/train/exp/weights/best.pt`.
Test it on an image:
```bash
python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.4 --source test.jpg
```
Test it on a video:
```bash
python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.4 --source video.mp4
```

---

## **7. Integrating the Model with YOLO**
To use the trained model in a Python script:
```python
import torch
import cv2
import numpy as np

# Load trained YOLO model
model = torch.hub.load('ultralytics/yolov5', 'custom', path='runs/train/exp/weights/best.pt', source='local')

# Load image
detect_img = cv2.imread('test.jpg')
results = model(detect_img)

# Display results
results.show()
```

---

## **4. ë§ˆìš°ìŠ¤ë¡œ ì„ íƒí•œ ê°ì²´ ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€**
YOLO ëª¨ë¸ì„ í™œìš©í•˜ì—¬ íƒì§€ëœ ê°ì²´ë¥¼ ë§ˆìš°ìŠ¤ë¡œ ì„ íƒí•˜ì—¬ ì¶”ì í•˜ëŠ” ê¸°ëŠ¥ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
import torch
import cv2
import numpy as np
from pyaidrone.aiDrone import *
from pyaidrone.deflib import *

# YOLO ëª¨ë¸ ë¡œë“œ (ì‚¬ìš©ì í•™ìŠµ ëª¨ë¸ ì ìš© ê°€ëŠ¥)
model = torch.hub.load('ultralytics/yolov5', 'custom', path='custom_model.pt', source='local')

# ë“œë¡  ì´ˆê¸°í™”
aidrone = AIDrone()
aidrone.Open("COM3")

# ì¹´ë©”ë¼ ì„¤ì •
cap = cv2.VideoCapture(0)
selected_object = None
selected_bbox = None

def select_object(event, x, y, flags, param):
    global selected_object, selected_bbox
    if event == cv2.EVENT_LBUTTONDOWN:
        for det in detections:
            x1, y1, x2, y2, conf, cls = det
            if x1 < x < x2 and y1 < y < y2:
                selected_object = model.names[int(cls)]
                selected_bbox = (x1, y1, x2, y2)
                print(f"Selected Object: {selected_object}")
                break

cv2.namedWindow("YOLO Object Tracking")
cv2.setMouseCallback("YOLO Object Tracking", select_object)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    results = model(frame)
    detections = results.xyxy[0].cpu().numpy()
    center_x, center_y = frame.shape[1] // 2, frame.shape[0] // 2
    target_x, target_y = None, None

    for det in detections:
        x1, y1, x2, y2, conf, cls = det
        label = model.names[int(cls)]
        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        cv2.putText(frame, label, (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        if selected_object and label == selected_object:
            target_x = (x1 + x2) / 2
            target_y = (y1 + y2) / 2

    if target_x and target_y:
        if target_x < center_x - 50:
            aidrone.velocity(LEFT, 50)  # ì™¼ìª½ ì´ë™
        elif target_x > center_x + 50:
            aidrone.velocity(RIGHT, 50)  # ì˜¤ë¥¸ìª½ ì´ë™
        else:
            aidrone.velocity(FRONT, 50)  # ì „ì§„
    else:
        aidrone.velocity(FRONT, 0)  # ë©ˆì¶¤

    cv2.imshow('YOLO Object Tracking', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
aidrone.Close()
```

ì´ì œ ë§ˆìš°ìŠ¤ë¡œ í´ë¦­í•˜ì—¬ ì›í•˜ëŠ” ê°ì²´ë¥¼ ì„ íƒí•˜ë©´, ë“œë¡ ì´ í•´ë‹¹ ê°ì²´ë¥¼ ë”°ë¼ê°€ë„ë¡ ë™ì‘í•˜ë©°, ìƒˆë¡œìš´ ê°ì²´ë¥¼ í•™ìŠµí•˜ì—¬ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

---

## **5. í”„ë¡œì íŠ¸ ìš”ì•½**
### **í•µì‹¬ ê¸°ëŠ¥**
- **YOLO ê¸°ë°˜ ê°ì²´ ê°ì§€**
- **ì‚¬ìš©ìê°€ ì„ íƒí•œ íŠ¹ì • ê°ì²´ë¥¼ ì¶”ì **
- **YOLOì— ì—†ëŠ” ê°ì²´ë¥¼ ì¶”ê°€ í•™ìŠµí•˜ì—¬ ì¶”ì  ê°€ëŠ¥**
- **ë“œë¡ ì´ ê°ì²´ë¥¼ ë”°ë¼ê°€ë„ë¡ ì œì–´**

### **í™•ì¥ ê°€ëŠ¥ì„±**
- **ì¶”ì  ì •í™•ë„ í–¥ìƒ (YOLOv7 ë“± ìµœì‹  ëª¨ë¸ ì ìš©)**
- **ë‹¤ì–‘í•œ ê°ì²´ íƒì§€ ë° ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€**
- **ê°•í™”í•™ìŠµì„ í™œìš©í•œ ìµœì  ê²½ë¡œ í•™ìŠµ**
- **GPS ë° LiDAR ì„¼ì„œì™€ ê²°í•©í•˜ì—¬ ì‹¤ì™¸ì—ì„œë„ í™œìš© ê°€ëŠ¥**

ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ YOLOë¥¼ ì´ìš©í•œ ê°ì²´ ê°ì§€ ë° ë“œë¡ ì˜ ìë™ ì¶”ì ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸš€


