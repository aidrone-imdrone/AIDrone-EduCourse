# 딥러닝 기초교육

## 목차

1. **딥러닝 개요**
   - 인공지능, 머신러닝, 딥러닝의 관계
   - 딥러닝의 역사와 발전
   - 딥러닝의 주요 응용 분야

2. **신경망의 기초**
   - 뉴런과 퍼셉트론 모델
   - 활성화 함수 (Activation Functions)
   - 순방향 신경망 (Feedforward Neural Networks)

3. **딥러닝 학습 원리**
   - 손실 함수 (Loss Functions)
   - 경사 하강법 (Gradient Descent)
   - 역전파 알고리즘 (Backpropagation)

4. **딥러닝 모델 종류**
   - CNN (합성곱 신경망)
   - RNN (순환 신경망)
   - Transformer 아키텍처

5. **실습 - 간단한 이미지 분류**
   - 데이터 준비 및 전처리
   - 모델 구현 및 학습
   - 평가 및 예측

---

## 1. 딥러닝 개요

### 인공지능, 머신러닝, 딥러닝의 관계

<img src="https://github.com/user-attachments/assets/fcadf4b2-aa60-4f1a-9dd7-0c7e34f8e3a0" width="500">

<br/>

- **인공지능(AI)**: 인간의 지능을 모방하는 기술 전반
- **머신러닝(ML)**: 데이터로부터 패턴을 학습하는 AI의 하위 분야
- **딥러닝(DL)**: 다층 신경망을 이용한 머신러닝의 하위 분야

### 딥러닝의 역사와 발전

- 1943: McCulloch & Pitts 인공 뉴런 모델 제안
- 1957: Rosenblatt의 퍼셉트론 알고리즘
- 1986: 역전파 알고리즘 개발
- 2006: Hinton의 "Deep Learning" 용어 사용 및 딥 빌리프 네트워크
- 2012: AlexNet - ImageNet 대회 우승으로 딥러닝 혁명 시작
- 2014-현재: 다양한 아키텍처 발전 (GAN, Transformer 등)

### 딥러닝의 주요 응용 분야

- 컴퓨터 비전 (이미지 인식, 객체 탐지)
- 자연어 처리 (기계 번역, 챗봇, 문서 요약)
- 음성 인식 및 합성
- 추천 시스템
- 게임 AI (알파고, 알파스타)
- 자율주행 자동차
- 의료 진단 및 약물 개발

---

## 2. 신경망의 기초

### 뉴런과 퍼셉트론 모델

<img src="https://github.com/user-attachments/assets/a2988f40-4629-4925-8437-ba47ff252c00" width="500">

<br/>

**인공 뉴런의 구성 요소**:
- 입력 (x₁, x₂, ..., xₙ)
- 가중치 (w₁, w₂, ..., wₙ)
- 편향 (b)
- 가중합 계산: z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
- 활성화 함수: y = f(z)

**퍼셉트론 학습 방법**:
- 입력에 대한 예측 수행
- 오차 계산 (실제값 - 예측값)
- 가중치 업데이트: w_new = w_old + α(실제값 - 예측값) × 입력값

### 활성화 함수 (Activation Functions)

**주요 활성화 함수**:

1. **Sigmoid**: f(x) = 1 / (1 + e^(-x))
   - 출력범위: 0~1
   - 장점: 확률로 해석 가능
   - 단점: 그래디언트 소멸 문제, 느린 계산

2. **Tanh**: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
   - 출력범위: -1~1
   - 장점: 중심값이 0, sigmoid보다 기울기가 큼
   - 단점: 여전히 깊은 네트워크에서 그래디언트 소멸 문제

3. **ReLU**: f(x) = max(0, x)
   - 출력범위: 0~∞
   - 장점: 계산 효율적, 그래디언트 소멸 문제 감소
   - 단점: Dying ReLU 문제 (0 이하의 입력에 대해 그래디언트가 0)

4. **Leaky ReLU**: f(x) = max(α·x, x), α는 작은 상수
   - 장점: Dying ReLU 문제 해결

### 순방향 신경망 (Feedforward Neural Networks)

![다층 퍼셉트론 구조]

- **입력층**: 데이터를 네트워크에 공급
- **은닉층**: 복잡한 특성을 학습 (깊이가 깊을수록 추상화 능력 향상)
- **출력층**: 최종 예측값 생성

**특징**:
- 각 노드는 이전 층의 모든 노드와 연결됨
- 정보는 입력에서 출력까지 한 방향으로만 전달
- 비선형 활성화 함수 덕분에 복잡한 패턴을 학습 가능

---

## 3. 딥러닝 학습 원리

### 손실 함수 (Loss Functions)

**손실 함수의 역할**:
- 모델의 예측값과 실제값 사이의 차이 측정
- 학습 과정을 통해 최소화해야 할 목표

**주요 손실 함수**:

1. **평균 제곱 오차 (MSE)**: 회귀 문제에 사용
   - L = (1/n) Σ(y - ŷ)²

2. **교차 엔트로피 손실 (Cross-Entropy Loss)**: 분류 문제에 사용
   - 이진 분류: L = -[y·log(ŷ) + (1-y)·log(1-ŷ)]
   - 다중 분류: L = -Σ y_i·log(ŷ_i)

### 경사 하강법 (Gradient Descent)

![경사 하강법 시각화]

**기본 원리**:
- 손실 함수를 최소화하는 방향으로 파라미터 업데이트
- 손실 함수의 그래디언트(기울기)를 계산
- 파라미터 업데이트 공식: θ_new = θ_old - η·∇L(θ)
  - θ: 모델 파라미터 (가중치, 편향)
  - η: 학습률 (learning rate)
  - ∇L(θ): 손실 함수의 그래디언트

**경사 하강법 종류**:
1. **배치 경사 하강법 (Batch GD)**: 전체 데이터셋 사용
2. **확률적 경사 하강법 (SGD)**: 한 번에 하나의 샘플 사용
3. **미니배치 경사 하강법 (Mini-batch GD)**: 작은 배치 단위로 사용

**최적화 알고리즘**:
- SGD with Momentum
- RMSprop
- Adam (가장 널리 사용)

### 역전파 알고리즘 (Backpropagation)

**역전파의 핵심 아이디어**:
- 출력층의 오차를 이용해 이전 층의 가중치를 효율적으로 업데이트
- 연쇄 법칙(Chain Rule)을 사용하여 그래디언트 계산
- 출력층에서 입력층 방향으로 오차를 전파

**역전파 단계**:
1. 순방향 전파: 입력 → 은닉층 → 출력층
2. 손실 계산: 예측값과 실제값 비교
3. 역방향 전파: 출력층 → 은닉층 → 입력층
4. 가중치 업데이트: 계산된 그래디언트를 이용해 파라미터 조정

---

## 4. 딥러닝 모델 종류

### CNN (합성곱 신경망)

![CNN 구조]

**주요 구성 요소**:
- **합성곱 층 (Convolutional Layer)**: 특성 추출
- **풀링 층 (Pooling Layer)**: 특성 다운샘플링
- **완전 연결 층 (Fully Connected Layer)**: 분류

**특징**:
- 이미지 데이터 처리에 최적화
- 파라미터 공유로 효율적인 학습
- 위치 불변성(translation invariance) 제공

**대표적 CNN 아키텍처**:
- LeNet, AlexNet, VGGNet, GoogLeNet, ResNet

### RNN (순환 신경망)

![RNN 구조]

**주요 특징**:
- 시퀀스 데이터 처리에 적합
- 내부 상태(internal state)를 유지하여 시간적 정보 저장
- 같은 파라미터를 시간 단계마다 재사용

**RNN의 한계**:
- 장기 의존성 문제 (Long-term dependency problem)
- 그래디언트 소실/폭발 문제

**발전된 RNN 아키텍처**:
- LSTM (Long Short-Term Memory)
- GRU (Gated Recurrent Unit)

**응용 분야**:
- 자연어 처리, 음성 인식, 시계열 예측

### Transformer 아키텍처

![Transformer 구조]

**주요 구성 요소**:
- **멀티헤드 셀프 어텐션 (Multi-head Self-attention)**
- **포지션 인코딩 (Positional Encoding)**
- **인코더-디코더 구조**

**특징**:
- 병렬 처리 가능 (RNN보다 훨씬 빠름)
- 긴 시퀀스의 의존성 문제 해결
- 컨텍스트 인식 능력이 뛰어남

**응용 분야**:
- 기계 번역 (원래 목적)
- BERT, GPT 등 최신 언어 모델의 기반
- 컴퓨터 비전으로 확장 (Vision Transformer)

---

## 5. 실습 - 간단한 이미지 분류

### 데이터 준비 및 전처리

**MNIST 데이터셋 로드 및 전처리**:
```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt

# 데이터 로드
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 데이터 정규화 (0-1 범위로 스케일링)
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# 차원 변환 (CNN 입력용)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# 레이블 원-핫 인코딩
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# 데이터 확인
print(f"학습 데이터 형태: {x_train.shape}")
print(f"테스트 데이터 형태: {x_test.shape}")
```

### 모델 구현 및 학습

**CNN 모델 구현**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# 모델 구성
model = Sequential([
    # 첫 번째 합성곱 층
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    
    # 두 번째 합성곱 층
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    
    # 특성 맵을 1차원 벡터로 변환
    Flatten(),
    
    # 완전 연결 층
    Dense(128, activation='relu'),
    Dropout(0.5),  # 과적합 방지
    
    # 출력층 (10개 클래스)
    Dense(10, activation='softmax')
])

# 모델 요약
model.summary()

# 모델 컴파일
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

**모델 학습**:
```python
# 모델 훈련
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.1,
    verbose=1
)

# 학습 과정 시각화
plt.figure(figsize=(12, 4))

# 정확도 그래프
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# 손실 그래프
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
```

### 평가 및 예측

**모델 평가**:
```python
# 테스트 세트에서 모델 평가
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"테스트 정확도: {test_acc:.4f}")
print(f"테스트 손실: {test_loss:.4f}")
```

**예측 및 시각화**:
```python
# 예측 수행
predictions = model.predict(x_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)

# 일부 예측 결과 시각화
plt.figure(figsize=(12, 8))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    
    if predicted_classes[i] == true_classes[i]:
        color = 'green'
    else:
        color = 'red'
    
    plt.title(f"예측: {predicted_classes[i]}, 실제: {true_classes[i]}", color=color)
    plt.axis('off')

plt.tight_layout()
plt.show()

# 혼동 행렬 시각화
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(true_classes, predicted_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()
```

---

## 딥러닝 학습 시 고려사항

### 과적합(Overfitting) 방지 기법
- 데이터 증강 (Data Augmentation)
- 드롭아웃 (Dropout)
- 배치 정규화 (Batch Normalization)
- 조기 종료 (Early Stopping)
- L1/L2 정규화 (Regularization)

### 하이퍼파라미터 튜닝
- 학습률 (Learning Rate)
- 배치 크기 (Batch Size)
- 은닉층 개수 및 뉴런 수
- 활성화 함수 선택
- 최적화 알고리즘 선택

### 실무 적용 시 고려사항
- 모델 복잡성과 데이터 크기의 균형
- 컴퓨팅 리소스 요구사항
- 추론 시간과 정확도의 트레이드오프
- 모델 해석 가능성 (Interpretability)
- 지속적인 모니터링 및 업데이트

---

## 참고 자료 및 추천 도서

### 온라인 강의
- Coursera: Deep Learning Specialization (Andrew Ng)
- Fast.ai: Practical Deep Learning for Coders
- Stanford CS231n: Convolutional Neural Networks for Visual Recognition

### 도서
- "Deep Learning" by Ian Goodfellow, Yoshua Bengio, Aaron Courville
- "Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow" by Aurélien Géron
- "Deep Learning with Python" by François Chollet

### 프레임워크 및 라이브러리
- TensorFlow: https://www.tensorflow.org/
- PyTorch: https://pytorch.org/
- Keras: https://keras.io/
