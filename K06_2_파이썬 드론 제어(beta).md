## 1. 이미지 분류를 통한 객체 인식 예제 (Object_detection.py)

### 목표
드론의 카메라로 촬영한 영상에서 특정 객체(예: 빨간 공)를 인식하고, 객체가 감지되면 드론이 특정 동작(예: 고도 증가)을 수행하도록 합니다.

### AI 개념
컴퓨터 비전: 카메라 영상에서 객체를 인식.
딥러닝: 사전 학습된 모델(MobileNet 등)을 사용해 객체를 분류.
실시간 처리: 드론의 카메라 스트림에서 실시간으로 데이터를 처리.

### 준비물
Raspberry Pi Zero 2W에 설치된 Python, OpenCV, TensorFlow Lite.
드론의 카메라 스트리밍 설정(첨부된 08_video.py 기반).
사전 학습된 MobileNet SSD 모델(TensorFlow Lite 버전).

### 코드 설명
드론 제어: 첨부된 02_altitude.py와 유사하게 드론을 이륙시키고, 고도를 조절합니다.
카메라 스트리밍: 08_video.py를 기반으로 카메라 스트림을 읽습니다.
객체 인식: TensorFlow Lite의 MobileNet SSD 모델을 사용해 실시간으로 객체를 감지합니다. 빨간 공(ball)이 감지되면 드론의 고도를 100cm로 설정합니다.
시각화: 감지된 객체에 바운딩 박스를 그리고, 결과를 화면에 표시합니다.

### 교육적 가치
학생들은 CNN 기반 객체 인식 모델의 작동 원리를 배우고, 실시간 데이터 처리의 중요성을 이해합니다.
Raspberry Pi의 제한된 리소스에서 딥러닝 모델을 실행하는 방법을 익힙니다.


## 2. 라인 트래킹 비행 예제 (line_tracking.py)

### 목표
드론의 카메라로 바닥의 검은 선을 인식하고, 선을 따라 비행하도록 합니다. 드론은 선의 위치에 따라 좌우로 이동하거나 회전합니다.

### AI 개념
이미지 처리: OpenCV를 사용해 선을 검출.
제어 알고리즘: PID 제어를 통해 드론의 이동을 안정화.
실시간 피드백: 카메라 데이터를 기반으로 즉각적인 제어 명령 생성.

### 준비물
Raspberry Pi Zero 2W에 설치된 Python, OpenCV.
드론의 카메라 스트리밍 설정.
바닥에 그려진 검은 선(예: 테이프로 만든 경로).

### 코드 설명
드론 제어: 드론을 이륙시키고 고도 50cm를 유지합니다(첨부된 02_altitude.py 기반).
카메라 처리: 카메라 영상에서 검은 선을 검출하기 위해 OpenCV의 이진화 및 컨투어 분석을 사용합니다.
PID 제어: 선의 중심이 화면 중심에서 벗어난 정도(error)를 기반으로 PID 제어를 적용해 드론의 좌우 이동을 조절합니다.
시각화: 검출된 선의 중심을 표시하고, 이진화된 영상을 별도로 출력합니다.

### 교육적 가치
학생들은 OpenCV를 사용한 기본적인 이미지 처리 기법을 배우고, PID 제어의 개념을 실제로 적용해봅니다.
실시간 피드백 루프와 센서 데이터 기반 제어의 중요성을 이해합니다.

### 추가 제안
#### 교육 난이도 조절
초보자: 라인 트래킹 비행 예제를 먼저 시도. 이미지 처리가 간단하고 직관적.
중급: 객체 인식 예제로 딥러닝 모델 활용법 학습.
고급: 두 예제를 결합해 "특정 객체를 따라가며 선을 트래킹"하는 복합 예제 구현.

### 확장 가능성
Raspberry Pi Zero 2W의 성능 제약을 고려해, 무거운 모델은 PC에서 실행하고 결과를 드론에 전송하는 클라이언트-서버 구조를 도입할 수 있습니다.
ToF 센서 데이터를 추가로 활용해 장애물 회피 기능을 구현하면 더 풍부한 AI 교육이 가능합니다.

### 교육 환경
간단한 테스트 공간(예: 2m x 2m 영역)에 테이프 선을 붙이고, 테스트용 객체(공 등)를 배치하세요.
학생들이 코드 수정 및 디버깅을 쉽게 할 수 있도록 주석과 로그를 풍부히 추가하세요.

## 3. 제스쳐 인식을 통한 드론 제어 (gesture_control.py)

### 목표
카메라로 촬영한 영상에서 사용자의 손 제스처를 인식합니다.
특정 제스처에 따라 드론의 동작을 제어합니다:

손바닥 열기(Open Hand): 이륙
주먹(Closed Fist): 착륙
검지와 중지 벌리기(V Sign): 전진
엄지 올리기(Thumbs Up): 우회전
엄지 내리기(Thumbs Down): 좌회전

### AI 개념
컴퓨터 비전: MediaPipe를 사용한 손 랜드마크 감지.
패턴 인식: 손가락의 위치와 상태를 분석해 제스처를 분류.
실시간 제어: 카메라 데이터를 기반으로 드론에 즉각적인 명령 전달.

### 준비물
Raspberry Pi Zero 2W에 설치된 Python, OpenCV, MediaPipe.
드론의 카메라 스트리밍 설정(제공된 08_video.py 기반).
pyaidrone 라이브러리(제공된 코드에서 사용 중).

### 설치 요구사항
MediaPipe는 Raspberry Pi Zero 2W에서 실행 가능하지만, 성능 최적화를 위해 아래 명령으로 설치하세요:

pip install opencv-python mediapipe


### 코드 설명
드론 제어: 제공된 pyaidrone 라이브러리를 사용해 이륙(takeoff), 착륙(landing), 전진(velocity), 회전(rotation)을 제어합니다. 기존 코드(04_rotation.py, 05_velocity.py, 07_key_control.py)에서 사용된 명령을 기반으로 구성했습니다.
카메라 처리: 08_video.py에서 사용된 카메라 스트리밍 설정(http://192.168.4.1/?action=stream)을 활용해 실시간 영상을 가져옵니다.

###제스처 인식:
MediaPipe의 Hands 모듈을 사용해 손의 랜드마크(21개 주요 포인트)를 추출합니다.
손가락의 상대적 위치(예: 펴졌는지 접혔는지)를 분석해 제스처를 분류합니다.
인식된 제스처는 화면에 텍스트로 표시되며, 해당 제스처에 맞는 드론 명령을 실행합니다.
실시간 처리: 프레임 크기를 640x480으로 조정해 Raspberry Pi Zero 2W의 성능 부담을 줄였습니다.
종료 처리: 예외 발생 시 드론을 안전하게 착륙시키고 카메라 및 MediaPipe 리소스를 정리합니다.

제스처 정의
제스처	설명	드론 동작
Open Hand	손바닥을 펴고 모든 손가락 펴기	이륙
Closed Fist	주먹을 쥐기	착륙
V Sign	검지와 중지만 펴기	전진 (50cm/s)
Thumbs Up	엄지를 위로	우회전 (30도)
Thumbs Down	엄지를 아래로	좌회전 (-30도)

### 사용 방법
Raspberry Pi Zero 2W에 코드를 업로드하고 실행합니다.
드론과 카메라가 제대로 연결되었는지 확인합니다(포트: COM3, 스트림: http://192.168.4.1/?action=stream).
카메라 앞에서 제스처를 취하면 드론이 해당 동작을 수행합니다.
ESC 키를 눌러 프로그램을 종료합니다.

### 교육적 가치
컴퓨터 비전: 학생들은 MediaPipe를 통해 손 랜드마크를 추출하고, 이를 기반으로 패턴을 인식하는 방법을 배웁니다.
실시간 제어: 카메라 데이터를 실시간으로 처리해 하드웨어(드론)를 제어하는 경험을 통해 AI와 로보틱스의 연계를 이해합니다.
창의성: 학생들이 새로운 제스처를 추가하거나 제어 로직을 수정하며 창의적인 프로젝트를 만들어볼 수 있습니다.

### 성능 최적화 팁
프레임 속도: Raspberry Pi Zero 2W는 성능이 제한적이므로, cv2.resize로 프레임 크기를 더 줄이거나(320x240), min_detection_confidence를 낮춰(예: 0.5) 감지 속도를 높일 수 있습니다.
단순화: 제스처 수를 줄이거나, 특정 손(오른손만)으로 제한하면 처리 부담이 감소합니다.
디버깅: 콘솔에 출력되는 제스처 로그를 확인하며 인식 정확도를 점검하세요.

### 추가 확장
다중 제스처: 두 손을 사용해 복잡한 명령(예: 한 손은 이동, 다른 손은 회전)을 구현할 수 있습니다.
거리 기반 제어: 손의 크기(카메라와의 거리)를 분석해 속도나 고도를 조절합니다.
모델 학습: MediaPipe 대신 사용자 정의 제스처를 학습한 경량 딥러닝 모델(TensorFlow Lite)을 사용할 수 있습니다.

## 4. 템플릿 매칭 드론 제어 (template_matching_control.py)

### 목표
미리 저장된 손 동작 이미지(예: 손바닥 열기, 주먹, V 사인)를 템플릿으로 사용.
드론의 카메라로 촬영한 실시간 손 동작을 템플릿과 비교.
템플릿과 일치하는 동작이 감지되면 드론이 특정 동작(이륙, 착륙, 이동 등)을 수행.

### AI 개념
컴퓨터 비전: OpenCV를 사용한 템플릿 매칭 또는 특징점 매칭(SIFT/ORB).
패턴 인식: 저장된 이미지와 실시간 이미지 간의 유사도를 계산.
실시간 제어: 매칭 결과를 기반으로 드론에 명령 전달.
구현 방식
Raspberry Pi Zero 2W의 성능을 고려해 템플릿 매칭을 기본으로 사용하겠습니다. 템플릿 매칭은 간단하고 이해하기 쉬우며, 초보자도 접근 가능한 기술입니다. 고급 옵션으로 ORB 특징점 매칭도 간단히 언급하겠습니다.

### 준비물
Raspberry Pi Zero 2W에 설치된 Python, OpenCV.
드론의 카메라 스트리밍 설정(제공된 08_video.py 기반).
미리 준비된 손 동작 이미지(예: open_hand.jpg, fist.jpg, v_sign.jpg).
pyaidrone 라이브러리.

### 설치 요구사항
OpenCV가 이미 설치되어 있어야 합니다. 설치되지 않았다면:

pip install opencv-python

### 손 동작 이미지 준비
드론의 카메라로 손 동작(손바닥 열기, 주먹, V 사인)을 촬영해 JPG 파일로 저장합니다.
예: open_hand.jpg, fist.jpg, v_sign.jpg.
동일한 조명 조건과 배경에서 촬영해 매칭 정확도를 높입니다.
이미지는 100x100 픽셀 정도로 크기를 조정해 성능을 최적화합니다.

### 코드 설명
드론 제어: 제공된 pyaidrone 라이브러리를 사용해 이륙(takeoff), 착륙(landing), 전진(velocity)을 제어합니다. 기존 코드(04_rotation.py, 05_velocity.py)에서 사용된 명령을 기반으로 구성했습니다.
카메라 처리: 08_video.py의 스트리밍 설정(http://192.168.4.1/?action=stream)을 활용해 실시간 영상을 가져옵니다.

### 템플릿 매칭:
OpenCV의 cv2.matchTemplate를 사용해 실시간 프레임과 저장된 템플릿 이미지를 비교합니다.
TM_CCOEFF_NORMED 방법을 사용해 정규화된 상관계수를 계산하며, 임계값(0.8) 이상일 때 매칭으로 간주합니다.
매칭된 경우, 해당 영역에 녹색 사각형을 그려 시각화합니다.

### 드론 동작:
open_hand: 드론 이륙, 비행 상태 플래그 설정.
fist: 드론 착륙, 비행 상태 플래그 해제.
v_sign: 드론 전진(50cm/s로 1초간 이동 후 정지).
상태 관리: is_flying 플래그를 사용해 이륙/착륙 상태를 추적하며, 중복 명령을 방지합니다.
종료 처리: 예외 발생 시 드론을 안전하게 착륙시키고 리소스를 정리합니다.

### 제스처 정의
템플릿	설명	드론 동작
open_hand	손바닥 열기	이륙
fist	주먹 쥐기	착륙
v_sign	검지와 중지 벌리기	전진 (50cm/s)

### 사용 방법
손 동작 이미지를 준비합니다:
드론의 카메라로 손바닥 열기, 주먹, V 사인을 촬영해 각각 open_hand.jpg, fist.jpg, v_sign.jpg로 저장.
이미지 크기를 100x100 정도로 조정해 저장.
코드를 Raspberry Pi Zero 2W에 업로드하고, 템플릿 이미지 파일을 코드와 같은 디렉토리에 배치합니다.
드론과 카메라가 연결된 상태에서 코드를 실행합니다.
카메라 앞에서 저장된 손 동작과 동일한 제스처를 취하면 드론이 동작합니다.
ESC 키를 눌러 프로그램을 종료합니다.

### 교육적 가치
컴퓨터 비전: 학생들은 템플릿 매칭의 기본 원리를 배우고, 이미지 간 유사도 계산 방법을 이해합니다.
실시간 제어: 실시간 영상 처리와 하드웨어 제어 간의 연계를 경험합니다.
창의성: 학생들이 새로운 손 동작 템플릿을 추가하거나, 매칭 임계값을 조정하며 실험해볼 수 있습니다.

### 성능 최적화 팁
템플릿 크기: 템플릿 이미지를 작게(50x50~100x100) 유지해 계산 속도를 높입니다.
프레임 해상도: 프레임 크기를 320x240으로 줄이면 Raspberry Pi Zero 2W의 부담이 감소합니다.
조명 조건: 템플릿과 실시간 영상의 조명 조건을 비슷하게 맞추면 매칭 정확도가 향상됩니다.
임계값 조정: threshold=0.8을 낮추거나 높여 매칭 민감도를 조절하세요.

### 고급 옵션: ORB 특징점 매칭
템플릿 매칭은 조명이나 회전에 민감할 수 있습니다. 더 견고한 매칭을 원한다면 ORB(OpenCV의 특징점 검출기)를 사용할 수 있습니다.

matched, loc = match_template(frame, template, threshold=0.8)

  =>  matched, loc = match_orb(frame, template, threshold=0.8)


## 5. 얼굴 추적 드론 (face_tracking_control.py)

### 목표
드론의 카메라로 사용자의 얼굴을 실시간으로 감지.
얼굴의 위치(화면 내 x, y 좌표)를 분석해 드론의 동작을 제어:
얼굴이 화면 중앙에서 좌우로 이동: 드론이 좌우로 이동하거나 회전.
얼굴이 화면 중앙에서 상하로 이동: 드론이 고도를 조절.
얼굴이 가까워지거나 멀어짐(크기 변화): 드론이 전진/후진.

### AI 개념
컴퓨터 비전: OpenCV를 사용한 얼굴 감지 및 위치 추적.
실시간 제어: 얼굴의 상대적 위치를 기반으로 드론의 속도 및 방향 제어.
피드백 루프: 얼굴 위치 데이터를 활용한 동적 드론 제어.

### 구현 방식
Raspberry Pi Zero 2W의 성능을 고려해 Haar Cascade 얼굴 감지기를 기본으로 사용합니다. Haar Cascade는 가볍고 초보자에게 이해하기 쉬우며, 교육용으로 적합합니다. 고급 옵션으로 DNN 기반 얼굴 감지기를 간단히 언급하겠습니다.

### 준비물
Raspberry Pi Zero 2W에 설치된 Python, OpenCV.
드론의 카메라 스트리밍 설정(제공된 08_video.py 기반: http://192.168.4.1/?action=stream).
pyaidrone 라이브러리.
OpenCV의 Haar Cascade XML 파일(haarcascade_frontalface_default.xml).

### 설치 요구사항
OpenCV가 설치되어 있어야 합니다:

### 코드 설명
드론 제어: 제공된 pyaidrone 라이브러리를 사용해 이륙(takeoff), 착륙(landing), 고도 조절(altitude), 속도 제어(velocity)를 구현합니다. 기존 코드(02_altitude.py, 05_velocity.py)에서 사용된 명령을 기반으로 합니다.
카메라 처리: 08_video.py의 스트리밍 설정(http://192.168.4.1/?action=stream)을 활용해 실시간 영상을 가져옵니다.
얼굴 감지:
OpenCV의 Haar Cascade(haarcascade_frontalface_default.xml)를 사용해 얼굴을 감지.
가장 큰 얼굴을 선택해 위치(x, y)와 크기(w, h)를 추출.
드론 동작:
이륙: 얼굴이 처음 감지되면 드론이 이륙하고 고도 100cm를 설정.
좌우 이동: 얼굴 중심(cx)이 화면 중앙에서 벗어나면 드론이 좌우로 이동(RIGHT 방향 속도 제어).
고도 조절: 얼굴 중심(cy)이 화면 중앙에서 위/아래로 벗어나면 고도를 50~150cm 범위로 조절.
전진/후진: 얼굴 크기(area)가 변화하면 전진/후진 속도를 조절(기준 면적 10000 기준).
상태 관리: is_flying 플래그로 이륙/착륙 상태를 추적하며, 얼굴이 감지되지 않을 때는 드론을 정지.
시각화: 감지된 얼굴에 녹색 사각형과 중심점을 표시하며, 상태 텍스트를 화면에 출력.
종료 처리: 예외 발생 시 드론을 착륙시키고 리소스를 정리.

### 제어 로직
얼굴 움직임	드론 동작
좌우 이동 (x_error)	좌우 이동 (RIGHT, ±50cm/s)
상하 이동 (y_error)	고도 조절 (50~150cm)
가까워짐/멀어짐 (area)	전진/후진 (FRONT, ±30cm/s)
얼굴 없음	모든 이동 정지

### 사용 방법
OpenCV의 Haar Cascade XML 파일이 코드와 같은 디렉토리에 있거나, OpenCV 데이터 경로에서 로드되는지 확인.
코드를 Raspberry Pi Zero 2W에 업로드하고 실행.
드론과 카메라가 연결된 상태(COM3, http://192.168.4.1/?action=stream)에서 카메라 앞에 얼굴을 위치.
얼굴을 좌우, 상하로 움직이거나 가까이/멀리 이동해 드론의 반응을 확인.
ESC 키를 눌러 프로그램 종료.
### 교육적 가치
컴퓨터 비전: 학생들은 Haar Cascade를 통한 얼굴 감지 원리를 배우고, 실시간 영상 처리의 기초를 이해.
피드백 제어: 얼굴 위치 데이터를 기반으로 드론의 동적 제어를 경험하며, 피드백 루프의 중요성 학습.
창의성: 학생들이 제어 로직(예: 회전 추가)이나 감지 조건을 수정하며 실험 가능.
### 성능 최적화 팁
프레임 해상도: frame_width=320, frame_height=240으로 줄이면 성능 향상.
감지 민감도: scaleFactor=1.2, minNeighbors=3으로 조정해 감지 속도와 정확도 균형 조절.
조명 조건: 균일한 조명에서 얼굴 감지 정확도가 높아짐.
임계값: x_error, y_error, area_diff의 임계값(50, 5000 등)을 조정해 제어 민감도 변경.

### 고급 옵션: DNN 기반 얼굴 감지
Haar Cascade는 조명이나 각도 변화에 민감할 수 있습니다. 더 정확한 감지를 원한다면 OpenCV의 DNN 모듈을 사용한 얼굴 감지기를 적용할 수 있습니다.
DNN 모델 파일(deploy.prototxt, caffemodel)을 다운로드해 사용.

wget https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt

wget https://raw.githubusercontent.com/opencv/opencv_3rdparty/ff8d3f3e7828ef4194d52f5dfb7a6fdf6bcdac06/opencv_face_detector/res10_300x300_ssd_iter_140000.caffemodel

