# 📌 1. 딥러닝 개요 (Introduction to Deep Learning)

## 1.1 인공지능(AI), 머신러닝(ML), 딥러닝(DL) 개념 차이

### 🔹 인공지능(AI, Artificial Intelligence)
인공지능(AI)은 **인간의 사고방식을 모방하는 컴퓨터 시스템**을 의미합니다. AI는 사람이 수행하는 작업을 자동화하고, 주어진 문제를 해결하도록 설계된 알고리즘을 포함합니다.  
AI는 크게 두 가지로 나뉩니다.

- **약한 AI(Weak AI)**: 특정 작업을 수행하는 AI (예: 음성 비서, 자율주행, 이미지 인식)
- **강한 AI(Strong AI)**: 인간처럼 사고하고 문제 해결을 할 수 있는 AI (아직 연구 중)

### 🔹 머신러닝(ML, Machine Learning)
머신러닝은 **AI의 한 분야**로, 데이터를 이용하여 **패턴을 학습**하고 스스로 **결정을 내릴 수 있는 모델**을 만드는 기술입니다.  
- 전통적인 프로그래밍과 달리 **규칙을 직접 명시하지 않고**, 데이터를 통해 학습합니다.
- 머신러닝 모델의 종류:
  - 지도 학습 (Supervised Learning): 레이블이 있는 데이터를 학습
  - 비지도 학습 (Unsupervised Learning): 레이블 없이 패턴을 발견
  - 강화 학습 (Reinforcement Learning): 보상을 기반으로 학습

### 🔹 딥러닝(DL, Deep Learning)
딥러닝은 **머신러닝의 한 분야**로, **다층 신경망(Deep Neural Networks, DNN)** 을 이용한 학습 방법입니다.  
- 전통적인 머신러닝은 특징을 사람이 직접 추출해야 하지만, 딥러닝은 **데이터에서 자동으로 특징(feature)을 학습**합니다.
- 딥러닝의 핵심 구조는 **인공 신경망(ANN, Artificial Neural Networks)** 이며, 대표적인 모델은 **CNN, RNN, Transformer** 등이 있습니다.

---

## 1.2 딥러닝의 역사 및 발전 과정

딥러닝의 발전 과정은 다음과 같이 요약할 수 있습니다.

| 연도 | 주요 발전 사항 |
|------|--------------|
| 1943 | 워렌 맥컬럭과 월터 피츠(Warren McCulloch & Walter Pitts)가 최초의 신경망 개념 제안 |
| 1957 | 프랭크 로젠블랫(Frank Rosenblatt)이 퍼셉트론(Perceptron) 모델 발표 |
| 1980s | 백프로파게이션(Backpropagation) 알고리즘 등장 (Geoffrey Hinton) |
| 1990s | CNN(Convolutional Neural Network) 개발 (LeNet-5 by Yann LeCun) |
| 2006 | 딥러닝이 주목받기 시작 (Geoffrey Hinton의 심층 신경망 연구) |
| 2012 | AlexNet(이미지 분류 모델)이 ImageNet 대회에서 압도적인 성능을 보이며 딥러닝이 급부상 |
| 2014 | GAN(Generative Adversarial Networks) 등장 |
| 2017 | 트랜스포머(Transformer) 모델 등장 (구글) |
| 2020s | GPT, DALL-E, Stable Diffusion 같은 대형 모델이 활발히 연구됨 |

---

## 1.3 딥러닝이 활용되는 분야

딥러닝은 다양한 분야에서 활용되고 있으며, 대표적인 응용 사례는 다음과 같습니다.

### ✅ 컴퓨터 비전 (Computer Vision)
- 이미지 분류 (Image Classification): 예) 사람 얼굴 인식, 의료 영상 분석
- 객체 검출 (Object Detection): 예) CCTV 보안, 자율주행
- 스타일 변환 (Style Transfer): 예) 화가의 그림 스타일을 이미지에 적용

### ✅ 자연어 처리 (NLP, Natural Language Processing)
- 기계 번역 (Machine Translation): 예) Google Translate
- 감정 분석 (Sentiment Analysis): 예) 리뷰에서 긍정/부정 판단
- 텍스트 생성 (Text Generation): 예) ChatGPT, BERT 기반 모델

### ✅ 자율주행 (Autonomous Driving)
- 차선 감지, 보행자 인식, 신호등 판별 등의 기술
- 테슬라(Tesla), 웨이모(Waymo) 같은 기업이 연구 중

### ✅ 추천 시스템 (Recommendation Systems)
- 넷플릭스, 유튜브, 아마존의 콘텐츠 추천

이 외에도 **의료 진단, 금융 예측, 드론 제어, 로봇 제어** 등의 다양한 분야에서 딥러닝이 활용됩니다.

---

## 1.4 왜 딥러닝이 중요한가?

딥러닝이 최근 급격히 발전한 이유는 **데이터, 연산력, 알고리즘의 발전** 덕분입니다.

### 📌 1️⃣ 데이터(Big Data)의 증가
- 스마트폰, 인터넷, IoT 기기의 발전으로 엄청난 양의 데이터가 생성됨
- 딥러닝 모델이 학습할 수 있는 충분한 데이터가 확보됨

### 📌 2️⃣ 연산력(Computational Power)의 향상
- GPU(그래픽 프로세서)의 발전으로 병렬 연산 가능
- TPU(Tensor Processing Unit) 같은 딥러닝 전용 하드웨어 등장

### 📌 3️⃣ 알고리즘의 개선
- CNN, RNN, Transformer 같은 효율적인 구조 등장
- 최적화 알고리즘(Adam, RMSprop 등)의 발전으로 학습 속도 증가

---

## 🛠 실습: 간단한 퍼셉트론 모델 직접 구현 (NumPy 사용)

퍼셉트론(Perceptron)은 가장 기본적인 신경망 모델입니다.  
아래 코드를 실행하여 퍼셉트론이 어떻게 작동하는지 실습할 수 있습니다.

### 🔹 퍼셉트론 구현 (AND 게이트 학습)

```python
import numpy as np

# AND 게이트 데이터셋
X = np.array([[0,0], [0,1], [1,0], [1,1]])  # 입력
y = np.array([0, 0, 0, 1])  # 정답

# 가중치 초기화
w = np.random.rand(2)
b = np.random.rand(1)
learning_rate = 0.1

# 퍼셉트론 학습
for epoch in range(10):  # 10번 반복
    for i in range(len(X)):
        z = np.dot(X[i], w) + b  # 가중치 곱 + 편향
        y_pred = 1 if z > 0 else 0  # 활성화 함수
        error = y[i] - y_pred
        w += learning_rate * error * X[i]  # 가중치 업데이트
        b += learning_rate * error  # 편향 업데이트

print("학습 완료된 가중치:", w)
print("학습 완료된 편향:", b)

<br/><br/>

# 📌 2. 신경망 기초 (Fundamentals of Neural Networks)

## 2.1 뉴런(Neuron)과 퍼셉트론(Perceptron)

### 🔹 뉴런(Neuron)의 개념
뉴런은 생물학적 신경망에서 영감을 얻어 만들어진 개념으로, **인공 신경망(Artificial Neural Networks, ANN)** 의 기본 단위입니다.  
뉴런은 여러 개의 입력을 받아 가중치를 곱하고, 편향(bias)을 더한 후, 활성화 함수를 통해 출력을 생성합니다.

수식으로 표현하면 다음과 같습니다.

\[
y = f(WX + b)
\]

여기서,
- \(X = (x_1, x_2, ..., x_n)\) : 입력 데이터
- \(W = (w_1, w_2, ..., w_n)\) : 가중치 (Weight)
- \(b\) : 편향 (Bias)
- \(f(\cdot)\) : 활성화 함수 (Activation Function)

---

### 🔹 퍼셉트론(Perceptron)
퍼셉트론은 가장 간단한 형태의 인공 신경망 모델로, 다음과 같이 동작합니다.
1. 여러 개의 입력을 받아 각각 가중치를 곱함.
2. 모든 값을 합산하고 편향(bias)을 더함.
3. 활성화 함수를 적용하여 최종 출력을 결정.

**퍼셉트론의 수식**
\[
y = \begin{cases} 
1, & \text{if } WX + b > 0 \\
0, & \text{otherwise}
\end{cases}
\]

퍼셉트론은 선형 분류 문제를 해결할 수 있으며, 다층 신경망(MLP)의 기초가 됩니다.

---

## 2.2 활성화 함수(Activation Functions) 이해

활성화 함수는 **신경망이 비선형 문제를 학습할 수 있도록 해주는 함수**입니다. 대표적인 활성화 함수는 다음과 같습니다.

### ✅ 시그모이드(Sigmoid) 함수
\[
f(x) = \frac{1}{1 + e^{-x}}
\]
- 출력값을 (0,1) 범위로 변환
- 문제점: 기울기 소실(Vanishing Gradient) 문제 발생

### ✅ 하이퍼볼릭 탄젠트(Tanh) 함수
\[
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
- 출력 범위 (-1,1)로 시그모이드보다 성능이 개선됨

### ✅ 렐루(ReLU) 함수
\[
f(x) = \max(0, x)
\]
- 음수값을 0으로 변환하여 계산량을 줄임
- 가장 널리 사용되는 활성화 함수

### ✅ 소프트맥스(Softmax) 함수
\[
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]
- 다중 클래스 분류 문제에서 사용됨

---

## 2.3 다층 신경망(Multi-layer Perceptron, MLP) 개념

다층 신경망(MLP)은 하나 이상의 **은닉층(Hidden Layer)** 을 포함하는 신경망입니다.

### 🔹 MLP 구조
1. **입력층 (Input Layer)**: 데이터 입력
2. **은닉층 (Hidden Layer)**: 비선형 변환 수행 (활성화 함수 적용)
3. **출력층 (Output Layer)**: 최종 결과 출력

### 🔹 MLP의 특징
- 퍼셉트론과 달리 **비선형 문제를 해결**할 수 있음
- 여러 층을 쌓으면 깊은 신경망(Deep Neural Networks)이 됨

---

## 2.4 순전파(Forward Propagation) 과정

순전파(Forward Propagation)는 입력 데이터가 신경망을 통과하여 최종 출력을 계산하는 과정입니다.

### 🔹 순전파 과정
1. 입력 데이터를 받아 각 층에서 가중치와 곱하고 편향을 더함
2. 활성화 함수를 적용하여 비선형 변환 수행
3. 최종 출력을 생성

\[
Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}
\]
\[
A^{(l)} = f(Z^{(l)})
\]

---

## 🛠 실습: 단순한 신경망 직접 구현 (Python + NumPy)

아래 코드는 단순한 **2층 신경망**을 구현한 예제입니다.

```python
import numpy as np

# 시그모이드 활성화 함수
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 신경망 초기화
np.random.seed(1)
W1 = np.random.randn(2, 3)  # 입력층 -> 은닉층 가중치
b1 = np.zeros((1, 3))       # 은닉층 편향
W2 = np.random.randn(3, 1)  # 은닉층 -> 출력층 가중치
b2 = np.zeros((1, 1))       # 출력층 편향

# 입력 데이터 (2개 특성)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# 순전파 과정
Z1 = np.dot(X, W1) + b1  # 첫 번째 층
A1 = sigmoid(Z1)         # 활성화 함수 적용
Z2 = np.dot(A1, W2) + b2  # 두 번째 층
A2 = sigmoid(Z2)         # 출력층 활성화 함수 적용

print("출력 결과:\n", A2)
