# ğŸ“Œ 1. ë”¥ëŸ¬ë‹ ê°œìš” (Introduction to Deep Learning)

## 1.1 ì¸ê³µì§€ëŠ¥(AI), ë¨¸ì‹ ëŸ¬ë‹(ML), ë”¥ëŸ¬ë‹(DL) ê°œë… ì°¨ì´

### ğŸ”¹ ì¸ê³µì§€ëŠ¥(AI, Artificial Intelligence)
ì¸ê³µì§€ëŠ¥(AI)ì€ **ì¸ê°„ì˜ ì‚¬ê³ ë°©ì‹ì„ ëª¨ë°©í•˜ëŠ” ì»´í“¨í„° ì‹œìŠ¤í…œ**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. AIëŠ” ì‚¬ëŒì´ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì„ ìë™í™”í•˜ê³ , ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ë„ë¡ ì„¤ê³„ëœ ì•Œê³ ë¦¬ì¦˜ì„ í¬í•¨í•©ë‹ˆë‹¤.  
AIëŠ” í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

- **ì•½í•œ AI(Weak AI)**: íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI (ì˜ˆ: ìŒì„± ë¹„ì„œ, ììœ¨ì£¼í–‰, ì´ë¯¸ì§€ ì¸ì‹)
- **ê°•í•œ AI(Strong AI)**: ì¸ê°„ì²˜ëŸ¼ ì‚¬ê³ í•˜ê³  ë¬¸ì œ í•´ê²°ì„ í•  ìˆ˜ ìˆëŠ” AI (ì•„ì§ ì—°êµ¬ ì¤‘)

### ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹(ML, Machine Learning)
ë¨¸ì‹ ëŸ¬ë‹ì€ **AIì˜ í•œ ë¶„ì•¼**ë¡œ, ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ **íŒ¨í„´ì„ í•™ìŠµ**í•˜ê³  ìŠ¤ìŠ¤ë¡œ **ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ” ëª¨ë¸**ì„ ë§Œë“œëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.  
- ì „í†µì ì¸ í”„ë¡œê·¸ë˜ë°ê³¼ ë‹¬ë¦¬ **ê·œì¹™ì„ ì§ì ‘ ëª…ì‹œí•˜ì§€ ì•Šê³ **, ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì¢…ë¥˜:
  - ì§€ë„ í•™ìŠµ (Supervised Learning): ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ë¥¼ í•™ìŠµ
  - ë¹„ì§€ë„ í•™ìŠµ (Unsupervised Learning): ë ˆì´ë¸” ì—†ì´ íŒ¨í„´ì„ ë°œê²¬
  - ê°•í™” í•™ìŠµ (Reinforcement Learning): ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ

### ğŸ”¹ ë”¥ëŸ¬ë‹(DL, Deep Learning)
ë”¥ëŸ¬ë‹ì€ **ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼**ë¡œ, **ë‹¤ì¸µ ì‹ ê²½ë§(Deep Neural Networks, DNN)** ì„ ì´ìš©í•œ í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤.  
- ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ì€ íŠ¹ì§•ì„ ì‚¬ëŒì´ ì§ì ‘ ì¶”ì¶œí•´ì•¼ í•˜ì§€ë§Œ, ë”¥ëŸ¬ë‹ì€ **ë°ì´í„°ì—ì„œ ìë™ìœ¼ë¡œ íŠ¹ì§•(feature)ì„ í•™ìŠµ**í•©ë‹ˆë‹¤.
- ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ êµ¬ì¡°ëŠ” **ì¸ê³µ ì‹ ê²½ë§(ANN, Artificial Neural Networks)** ì´ë©°, ëŒ€í‘œì ì¸ ëª¨ë¸ì€ **CNN, RNN, Transformer** ë“±ì´ ìˆìŠµë‹ˆë‹¤.

---

## 1.2 ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ ë° ë°œì „ ê³¼ì •

ë”¥ëŸ¬ë‹ì˜ ë°œì „ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| ì—°ë„ | ì£¼ìš” ë°œì „ ì‚¬í•­ |
|------|--------------|
| 1943 | ì›Œë Œ ë§¥ì»¬ëŸ­ê³¼ ì›”í„° í”¼ì¸ (Warren McCulloch & Walter Pitts)ê°€ ìµœì´ˆì˜ ì‹ ê²½ë§ ê°œë… ì œì•ˆ |
| 1957 | í”„ë­í¬ ë¡œì  ë¸”ë«(Frank Rosenblatt)ì´ í¼ì…‰íŠ¸ë¡ (Perceptron) ëª¨ë¸ ë°œí‘œ |
| 1980s | ë°±í”„ë¡œíŒŒê²Œì´ì…˜(Backpropagation) ì•Œê³ ë¦¬ì¦˜ ë“±ì¥ (Geoffrey Hinton) |
| 1990s | CNN(Convolutional Neural Network) ê°œë°œ (LeNet-5 by Yann LeCun) |
| 2006 | ë”¥ëŸ¬ë‹ì´ ì£¼ëª©ë°›ê¸° ì‹œì‘ (Geoffrey Hintonì˜ ì‹¬ì¸µ ì‹ ê²½ë§ ì—°êµ¬) |
| 2012 | AlexNet(ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸)ì´ ImageNet ëŒ€íšŒì—ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì´ë©° ë”¥ëŸ¬ë‹ì´ ê¸‰ë¶€ìƒ |
| 2014 | GAN(Generative Adversarial Networks) ë“±ì¥ |
| 2017 | íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸ ë“±ì¥ (êµ¬ê¸€) |
| 2020s | GPT, DALL-E, Stable Diffusion ê°™ì€ ëŒ€í˜• ëª¨ë¸ì´ í™œë°œíˆ ì—°êµ¬ë¨ |

---

## 1.3 ë”¥ëŸ¬ë‹ì´ í™œìš©ë˜ëŠ” ë¶„ì•¼

ë”¥ëŸ¬ë‹ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìœ¼ë©°, ëŒ€í‘œì ì¸ ì‘ìš© ì‚¬ë¡€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

### âœ… ì»´í“¨í„° ë¹„ì „ (Computer Vision)
- ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification): ì˜ˆ) ì‚¬ëŒ ì–¼êµ´ ì¸ì‹, ì˜ë£Œ ì˜ìƒ ë¶„ì„
- ê°ì²´ ê²€ì¶œ (Object Detection): ì˜ˆ) CCTV ë³´ì•ˆ, ììœ¨ì£¼í–‰
- ìŠ¤íƒ€ì¼ ë³€í™˜ (Style Transfer): ì˜ˆ) í™”ê°€ì˜ ê·¸ë¦¼ ìŠ¤íƒ€ì¼ì„ ì´ë¯¸ì§€ì— ì ìš©

### âœ… ìì—°ì–´ ì²˜ë¦¬ (NLP, Natural Language Processing)
- ê¸°ê³„ ë²ˆì—­ (Machine Translation): ì˜ˆ) Google Translate
- ê°ì • ë¶„ì„ (Sentiment Analysis): ì˜ˆ) ë¦¬ë·°ì—ì„œ ê¸ì •/ë¶€ì • íŒë‹¨
- í…ìŠ¤íŠ¸ ìƒì„± (Text Generation): ì˜ˆ) ChatGPT, BERT ê¸°ë°˜ ëª¨ë¸

### âœ… ììœ¨ì£¼í–‰ (Autonomous Driving)
- ì°¨ì„  ê°ì§€, ë³´í–‰ì ì¸ì‹, ì‹ í˜¸ë“± íŒë³„ ë“±ì˜ ê¸°ìˆ 
- í…ŒìŠ¬ë¼(Tesla), ì›¨ì´ëª¨(Waymo) ê°™ì€ ê¸°ì—…ì´ ì—°êµ¬ ì¤‘

### âœ… ì¶”ì²œ ì‹œìŠ¤í…œ (Recommendation Systems)
- ë„·í”Œë¦­ìŠ¤, ìœ íŠœë¸Œ, ì•„ë§ˆì¡´ì˜ ì½˜í…ì¸  ì¶”ì²œ

ì´ ì™¸ì—ë„ **ì˜ë£Œ ì§„ë‹¨, ê¸ˆìœµ ì˜ˆì¸¡, ë“œë¡  ì œì–´, ë¡œë´‡ ì œì–´** ë“±ì˜ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë”¥ëŸ¬ë‹ì´ í™œìš©ë©ë‹ˆë‹¤.

---

## 1.4 ì™œ ë”¥ëŸ¬ë‹ì´ ì¤‘ìš”í•œê°€?

ë”¥ëŸ¬ë‹ì´ ìµœê·¼ ê¸‰ê²©íˆ ë°œì „í•œ ì´ìœ ëŠ” **ë°ì´í„°, ì—°ì‚°ë ¥, ì•Œê³ ë¦¬ì¦˜ì˜ ë°œì „** ë•ë¶„ì…ë‹ˆë‹¤.

### ğŸ“Œ 1ï¸âƒ£ ë°ì´í„°(Big Data)ì˜ ì¦ê°€
- ìŠ¤ë§ˆíŠ¸í°, ì¸í„°ë„·, IoT ê¸°ê¸°ì˜ ë°œì „ìœ¼ë¡œ ì—„ì²­ë‚œ ì–‘ì˜ ë°ì´í„°ê°€ ìƒì„±ë¨
- ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ë°ì´í„°ê°€ í™•ë³´ë¨

### ğŸ“Œ 2ï¸âƒ£ ì—°ì‚°ë ¥(Computational Power)ì˜ í–¥ìƒ
- GPU(ê·¸ë˜í”½ í”„ë¡œì„¸ì„œ)ì˜ ë°œì „ìœ¼ë¡œ ë³‘ë ¬ ì—°ì‚° ê°€ëŠ¥
- TPU(Tensor Processing Unit) ê°™ì€ ë”¥ëŸ¬ë‹ ì „ìš© í•˜ë“œì›¨ì–´ ë“±ì¥

### ğŸ“Œ 3ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ì˜ ê°œì„ 
- CNN, RNN, Transformer ê°™ì€ íš¨ìœ¨ì ì¸ êµ¬ì¡° ë“±ì¥
- ìµœì í™” ì•Œê³ ë¦¬ì¦˜(Adam, RMSprop ë“±)ì˜ ë°œì „ìœ¼ë¡œ í•™ìŠµ ì†ë„ ì¦ê°€

---

## ğŸ›  ì‹¤ìŠµ: ê°„ë‹¨í•œ í¼ì…‰íŠ¸ë¡  ëª¨ë¸ ì§ì ‘ êµ¬í˜„ (NumPy ì‚¬ìš©)

í¼ì…‰íŠ¸ë¡ (Perceptron)ì€ ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹ ê²½ë§ ëª¨ë¸ì…ë‹ˆë‹¤.  
ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í¼ì…‰íŠ¸ë¡ ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‹¤ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”¹ í¼ì…‰íŠ¸ë¡  êµ¬í˜„ (AND ê²Œì´íŠ¸ í•™ìŠµ)

```python
import numpy as np

# AND ê²Œì´íŠ¸ ë°ì´í„°ì…‹
X = np.array([[0,0], [0,1], [1,0], [1,1]])  # ì…ë ¥
y = np.array([0, 0, 0, 1])  # ì •ë‹µ

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
w = np.random.rand(2)
b = np.random.rand(1)
learning_rate = 0.1

# í¼ì…‰íŠ¸ë¡  í•™ìŠµ
for epoch in range(10):  # 10ë²ˆ ë°˜ë³µ
    for i in range(len(X)):
        z = np.dot(X[i], w) + b  # ê°€ì¤‘ì¹˜ ê³± + í¸í–¥
        y_pred = 1 if z > 0 else 0  # í™œì„±í™” í•¨ìˆ˜
        error = y[i] - y_pred
        w += learning_rate * error * X[i]  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        b += learning_rate * error  # í¸í–¥ ì—…ë°ì´íŠ¸

print("í•™ìŠµ ì™„ë£Œëœ ê°€ì¤‘ì¹˜:", w)
print("í•™ìŠµ ì™„ë£Œëœ í¸í–¥:", b)

<br/><br/>

# ğŸ“Œ 2. ì‹ ê²½ë§ ê¸°ì´ˆ (Fundamentals of Neural Networks)

## 2.1 ë‰´ëŸ°(Neuron)ê³¼ í¼ì…‰íŠ¸ë¡ (Perceptron)

### ğŸ”¹ ë‰´ëŸ°(Neuron)ì˜ ê°œë…
ë‰´ëŸ°ì€ ìƒë¬¼í•™ì  ì‹ ê²½ë§ì—ì„œ ì˜ê°ì„ ì–»ì–´ ë§Œë“¤ì–´ì§„ ê°œë…ìœ¼ë¡œ, **ì¸ê³µ ì‹ ê²½ë§(Artificial Neural Networks, ANN)** ì˜ ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤.  
ë‰´ëŸ°ì€ ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ì„ ë°›ì•„ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ê³ , í¸í–¥(bias)ì„ ë”í•œ í›„, í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µí•´ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

\[
y = f(WX + b)
\]

ì—¬ê¸°ì„œ,
- \(X = (x_1, x_2, ..., x_n)\) : ì…ë ¥ ë°ì´í„°
- \(W = (w_1, w_2, ..., w_n)\) : ê°€ì¤‘ì¹˜ (Weight)
- \(b\) : í¸í–¥ (Bias)
- \(f(\cdot)\) : í™œì„±í™” í•¨ìˆ˜ (Activation Function)

---

### ğŸ”¹ í¼ì…‰íŠ¸ë¡ (Perceptron)
í¼ì…‰íŠ¸ë¡ ì€ ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤.
1. ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ì„ ë°›ì•„ ê°ê° ê°€ì¤‘ì¹˜ë¥¼ ê³±í•¨.
2. ëª¨ë“  ê°’ì„ í•©ì‚°í•˜ê³  í¸í–¥(bias)ì„ ë”í•¨.
3. í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ê²°ì •.

**í¼ì…‰íŠ¸ë¡ ì˜ ìˆ˜ì‹**
\[
y = \begin{cases} 
1, & \text{if } WX + b > 0 \\
0, & \text{otherwise}
\end{cases}
\]

í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜• ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì¸µ ì‹ ê²½ë§(MLP)ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤.

---

## 2.2 í™œì„±í™” í•¨ìˆ˜(Activation Functions) ì´í•´

í™œì„±í™” í•¨ìˆ˜ëŠ” **ì‹ ê²½ë§ì´ ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” í•¨ìˆ˜**ì…ë‹ˆë‹¤. ëŒ€í‘œì ì¸ í™œì„±í™” í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

### âœ… ì‹œê·¸ëª¨ì´ë“œ(Sigmoid) í•¨ìˆ˜
\[
f(x) = \frac{1}{1 + e^{-x}}
\]
- ì¶œë ¥ê°’ì„ (0,1) ë²”ìœ„ë¡œ ë³€í™˜
- ë¬¸ì œì : ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient) ë¬¸ì œ ë°œìƒ

### âœ… í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸(Tanh) í•¨ìˆ˜
\[
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
- ì¶œë ¥ ë²”ìœ„ (-1,1)ë¡œ ì‹œê·¸ëª¨ì´ë“œë³´ë‹¤ ì„±ëŠ¥ì´ ê°œì„ ë¨

### âœ… ë ë£¨(ReLU) í•¨ìˆ˜
\[
f(x) = \max(0, x)
\]
- ìŒìˆ˜ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°ëŸ‰ì„ ì¤„ì„
- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜

### âœ… ì†Œí”„íŠ¸ë§¥ìŠ¤(Softmax) í•¨ìˆ˜
\[
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]
- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì‚¬ìš©ë¨

---

## 2.3 ë‹¤ì¸µ ì‹ ê²½ë§(Multi-layer Perceptron, MLP) ê°œë…

ë‹¤ì¸µ ì‹ ê²½ë§(MLP)ì€ í•˜ë‚˜ ì´ìƒì˜ **ì€ë‹‰ì¸µ(Hidden Layer)** ì„ í¬í•¨í•˜ëŠ” ì‹ ê²½ë§ì…ë‹ˆë‹¤.

### ğŸ”¹ MLP êµ¬ì¡°
1. **ì…ë ¥ì¸µ (Input Layer)**: ë°ì´í„° ì…ë ¥
2. **ì€ë‹‰ì¸µ (Hidden Layer)**: ë¹„ì„ í˜• ë³€í™˜ ìˆ˜í–‰ (í™œì„±í™” í•¨ìˆ˜ ì ìš©)
3. **ì¶œë ¥ì¸µ (Output Layer)**: ìµœì¢… ê²°ê³¼ ì¶œë ¥

### ğŸ”¹ MLPì˜ íŠ¹ì§•
- í¼ì…‰íŠ¸ë¡ ê³¼ ë‹¬ë¦¬ **ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•´ê²°**í•  ìˆ˜ ìˆìŒ
- ì—¬ëŸ¬ ì¸µì„ ìŒ“ìœ¼ë©´ ê¹Šì€ ì‹ ê²½ë§(Deep Neural Networks)ì´ ë¨

---

## 2.4 ìˆœì „íŒŒ(Forward Propagation) ê³¼ì •

ìˆœì „íŒŒ(Forward Propagation)ëŠ” ì…ë ¥ ë°ì´í„°ê°€ ì‹ ê²½ë§ì„ í†µê³¼í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

### ğŸ”¹ ìˆœì „íŒŒ ê³¼ì •
1. ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ê° ì¸µì—ì„œ ê°€ì¤‘ì¹˜ì™€ ê³±í•˜ê³  í¸í–¥ì„ ë”í•¨
2. í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ë¹„ì„ í˜• ë³€í™˜ ìˆ˜í–‰
3. ìµœì¢… ì¶œë ¥ì„ ìƒì„±

\[
Z^{(l)} = W^{(l)} A^{(l-1)} + b^{(l)}
\]
\[
A^{(l)} = f(Z^{(l)})
\]

---

## ğŸ›  ì‹¤ìŠµ: ë‹¨ìˆœí•œ ì‹ ê²½ë§ ì§ì ‘ êµ¬í˜„ (Python + NumPy)

ì•„ë˜ ì½”ë“œëŠ” ë‹¨ìˆœí•œ **2ì¸µ ì‹ ê²½ë§**ì„ êµ¬í˜„í•œ ì˜ˆì œì…ë‹ˆë‹¤.

```python
import numpy as np

# ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ì‹ ê²½ë§ ì´ˆê¸°í™”
np.random.seed(1)
W1 = np.random.randn(2, 3)  # ì…ë ¥ì¸µ -> ì€ë‹‰ì¸µ ê°€ì¤‘ì¹˜
b1 = np.zeros((1, 3))       # ì€ë‹‰ì¸µ í¸í–¥
W2 = np.random.randn(3, 1)  # ì€ë‹‰ì¸µ -> ì¶œë ¥ì¸µ ê°€ì¤‘ì¹˜
b2 = np.zeros((1, 1))       # ì¶œë ¥ì¸µ í¸í–¥

# ì…ë ¥ ë°ì´í„° (2ê°œ íŠ¹ì„±)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# ìˆœì „íŒŒ ê³¼ì •
Z1 = np.dot(X, W1) + b1  # ì²« ë²ˆì§¸ ì¸µ
A1 = sigmoid(Z1)         # í™œì„±í™” í•¨ìˆ˜ ì ìš©
Z2 = np.dot(A1, W2) + b2  # ë‘ ë²ˆì§¸ ì¸µ
A2 = sigmoid(Z2)         # ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ ì ìš©

print("ì¶œë ¥ ê²°ê³¼:\n", A2)
